20-Nov-23 16:45:25 - Namespace(train_batch_size=8, max_seq_length=300, model_name='distilbert-base-uncased', max_passages=0, epochs=2, pooling='mean', negs_to_use=None, warmup_steps=1000, lr=2e-05, num_negs_per_system=5, use_pre_trained_model=False, use_all_queries=False, ce_score_margin=3.0, evaluation_steps=100)
20-Nov-23 16:45:25 - Create new SBERT model
20-Nov-23 16:45:30 - Use pytorch device: cuda
20-Nov-23 16:45:30 - Read corpus: collection.tsv
20-Nov-23 16:45:46 - Load CrossEncoder scores dict
20-Nov-23 16:46:24 - Loading data
20-Nov-23 16:46:31 - Mean loss for training step 100 :  0.20962451077997685
20-Nov-23 16:46:37 - Mean loss for training step 200 :  0.17463447518646716
20-Nov-23 16:46:42 - Mean loss for training step 300 :  0.13492932718247175
20-Nov-23 16:46:47 - Mean loss for training step 400 :  0.11029137508943677
20-Nov-23 16:46:53 - Mean loss for training step 500 :  0.10476671587675809
20-Nov-23 16:46:58 - Mean loss for training step 600 :  0.08870958178304136
20-Nov-23 16:47:04 - Mean loss for training step 700 :  0.08893415872007608
20-Nov-23 16:47:09 - Mean loss for training step 800 :  0.09084417287260294
20-Nov-23 16:47:15 - Mean loss for training step 900 :  0.08112656680867075
20-Nov-23 16:47:21 - Mean loss for training step 1000 :  0.07583451461978256
20-Nov-23 16:47:27 - Mean loss for training step 1100 :  0.07366393733769655
20-Nov-23 16:47:32 - Mean loss for training step 1200 :  0.0720409525372088
20-Nov-23 16:47:38 - Mean loss for training step 1300 :  0.06786502989009023
20-Nov-23 16:47:43 - Mean loss for training step 1400 :  0.06171908533666283
20-Nov-23 16:47:51 - Mean loss for training step 1500 :  0.061600394789129494
20-Nov-23 16:47:59 - Mean loss for training step 1600 :  0.06088786507025361
20-Nov-23 16:48:06 - Mean loss for training step 1700 :  0.05803161283954978
20-Nov-23 16:48:14 - Mean loss for training step 1800 :  0.05268102821428329
20-Nov-23 16:48:21 - Mean loss for training step 1900 :  0.05412765891756863
20-Nov-23 16:48:28 - Mean loss for training step 2000 :  0.05792567476863041
20-Nov-23 16:48:36 - Mean loss for training step 2100 :  0.04798735830932856
20-Nov-23 16:48:43 - Mean loss for training step 2200 :  0.05012190462090075
20-Nov-23 16:48:51 - Mean loss for training step 2300 :  0.051533615649677816
20-Nov-23 16:48:58 - Mean loss for training step 2400 :  0.04997279794188216
20-Nov-23 16:49:04 - Mean loss for training step 2500 :  0.04470579492393881
20-Nov-23 16:49:12 - Mean loss for training step 2600 :  0.04268203952815384
20-Nov-23 16:49:12 - Mean loss for epoch:  0.07942231821488797
20-Nov-23 16:49:20 - Mean loss for training step 100 :  0.03256935242097825
20-Nov-23 16:49:27 - Mean loss for training step 200 :  0.03266285730525851
20-Nov-23 16:49:35 - Mean loss for training step 300 :  0.03591754030669108
20-Nov-23 16:49:41 - Mean loss for training step 400 :  0.03531774789327755
20-Nov-23 16:49:49 - Mean loss for training step 500 :  0.031560803182655944
20-Nov-23 16:49:56 - Mean loss for training step 600 :  0.03045778746018186
20-Nov-23 16:50:04 - Mean loss for training step 700 :  0.03305013925069943
20-Nov-23 16:50:10 - Mean loss for training step 800 :  0.029896265453426166
20-Nov-23 16:50:15 - Mean loss for training step 900 :  0.030204951553605495
20-Nov-23 16:50:20 - Mean loss for training step 1000 :  0.03722258497728035
20-Nov-23 16:50:25 - Mean loss for training step 1100 :  0.03441119586816058
20-Nov-23 16:50:30 - Mean loss for training step 1200 :  0.030446194579126312
20-Nov-23 16:50:34 - Mean loss for training step 1300 :  0.028401842691237106
20-Nov-23 16:50:40 - Mean loss for training step 1400 :  0.026238140370696782
20-Nov-23 16:50:45 - Mean loss for training step 1500 :  0.02725149678532034
20-Nov-23 16:50:50 - Mean loss for training step 1600 :  0.028887946605682373
20-Nov-23 16:50:55 - Mean loss for training step 1700 :  0.02551407363032922
20-Nov-23 16:51:00 - Mean loss for training step 1800 :  0.03338331627659499
20-Nov-23 16:51:05 - Mean loss for training step 1900 :  0.027251981315785087
20-Nov-23 16:51:10 - Mean loss for training step 2000 :  0.027488066364312544
20-Nov-23 16:51:15 - Mean loss for training step 2100 :  0.026576120837125926
20-Nov-23 16:51:20 - Mean loss for training step 2200 :  0.02644808902987279
20-Nov-23 16:51:25 - Mean loss for training step 2300 :  0.025664791625458747
20-Nov-23 16:51:30 - Mean loss for training step 2400 :  0.025147727204021066
20-Nov-23 16:51:35 - Mean loss for training step 2500 :  0.028603613760787995
20-Nov-23 16:51:40 - Mean loss for training step 2600 :  0.026462419163435696
20-Nov-23 16:51:41 - Mean loss for epoch:  0.029860390500432304
20-Nov-23 16:51:41 - Save model to output/train_bi-encoder-margin_mse-distilbert-base-uncased-batch_size_8-2023-11-20_16-45-30/5212
20-Nov-23 16:51:43 - Save model to output/train_bi-encoder-margin_mse-distilbert-base-uncased-batch_size_8-2023-11-20_16-45-30
20-Nov-23 16:51:46 - Save model to output/train_bi-encoder-margin_mse-distilbert-base-uncased-batch_size_8-2023-11-20_16-45-30
